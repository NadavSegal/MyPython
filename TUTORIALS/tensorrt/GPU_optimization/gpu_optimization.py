# -*- coding: utf-8 -*-
"""https://towardsdatascience.com/optimize-nvidia-gpu-performance-for-efficient-model-inference-f3e9874e9fdc"""
"""GPU_optimization.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10ah6t0I2-MV_3uPqw6J_WhMHlfLflrr8

## Optimize inference performance for TensorFlow models on NVIDIA GPU devices

This document provide source code and instructions to reproduce results show in this article. Please run this notebook with a GPU accelerator. To do this, select *Edit -> Notebook Settings*, from the *Hardware accelerator* drop-down list, select GPU and save. This notebook contains the following sections: 

1. Install prerequisites
2. Benchmark raw model in TensorFlow
3. Optimize and benchmark optimized model in TensorFlow
4. Convert TensorFlow models to TensorRT and benchmark the model in TensorRT
5. Compare the resutls and conclusion


### Install prerequisites 

In this section, we will install and build all necessary packages to run this experiment. The following script will install the following packages and their dependencies:

* TensorFlow Object Detection API
* TensorRT 
* UFF Converter
* Graph Surgeon
* PyCUDA

**Note: Donwloading TensorRT packages requires login to [NVIDIA Website](https://developer.nvidia.com/nvidia-tensorrt-download). So, before running the script, you have to manually download it and upload to your google drive. Then change  file path in the script (default to *My Drive/tensorrt/filename.deb*) to your package path. After installing all packages, please restart the runtime in order for the kernel to recognize newly installed packages.**
"""

import os
from termcolor import cprint

# Install Tensorflow object detection API
# Clone models repo
!git clone https://github.com/tensorflow/models.git

# Compile protobuf 
os.chdir("models/research/")
!protoc object_detection/protos/*.proto --python_out=.
os.chdir("/content")

# ===============================================================
# Install TensorRT and its dependencies, note that you may need to restart
# the run time to use these libraries

from google.colab import drive

drive.mount("/content/gdrive")

# IMPORTANT: To make the script work properly, you need to donwload TensorRT
# from https://developer.nvidia.com/nvidia-tensorrt-download and upload the .deb
# file to your google drive. Then replcae the following path with the path to
# the .deb file. TensorRT 5.1.5.0 GA for Ubuntu 18.04 and CUDA 10.0 is tested
# on Colab, if you plan to use this script on other enviroment, please choose
# TensorRT version accordingly.
!sudo dpkg -i /content/gdrive/My\ Drive/tensorrt/nv-tensorrt-repo-ubuntu1804-cuda10.0-trt5.1.5.0-ga-20190427_1-1_amd64.deb

# Explicitly install dependencics
!sudo apt-get install -y --no-install-recommends libnvinfer5=5.1.5-1+cuda10.0
!sudo apt-get install -y --no-install-recommends libnvinfer-dev=5.1.5-1+cuda10.0

# Install TensorRT
!sudo apt-key add /var/nv-tensorrt-repo-cuda10.0-trt5.1.5.0-ga-20190427/7fa2af80.pub
!sudo apt-get update
!sudo apt-get install tensorrt
!sudo apt-get install python3-libnvinfer-dev
!sudo apt-get install uff-converter-tf

# Install pycuda
!pip3 install pycuda

# Build necessary plugin library
!cp -r /usr/src/tensorrt/samples/python/uff_ssd/plugin/ .
!cp -r /usr/src/tensorrt/samples/python/uff_ssd/CMakeLists.txt .
!mkdir build
os.chdir("build")
!cmake ..
!make
os.chdir("/content")

cprint("Finished install necessary packages, please restart the runtime now...", "red")

"""### Benchmark raw model in TensorFlow

Now, we will download pre-trained model from TensorFlow Object Detection API, export and run it without any modification. In this experiment, we will use **SSD MobileNet V2** model, which takes 300x300 images as input, perform object detection and returns bonding boxes, classes and confidence scores as outputs. The model is trained with COCO dataset. The following script downloads the model from TensorFlow.
"""

# Download SSD MobileNet V2 model
!wget http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v2_coco_2018_03_29.tar.gz
!tar -zxf ssd_mobilenet_v2_coco_2018_03_29.tar.gz

# Delete the batch_norm_trainable line in the config file as it cannot be
# recognized by exporter
!sed -i '/batch_norm_trainable/d' ssd_mobilenet_v2_coco_2018_03_29/pipeline.config

"""After downloading the model, we export the model using `pipeline.config` and checkpoint files using TensorFlow Object Detection Exporter."""

import tensorflow as tf
import numpy as np
import sys


# Add to path
sys.path.append("/content/models/research")
sys.path.append("/content/models/research/slim")

from object_detection import exporter
from pathlib import Path
from object_detection.protos import pipeline_pb2
from google.protobuf import text_format



# Define some helper functions here
def export_detection_model(model_dir: str):
    """
    Export model given model directory
    Args:
        model_dir: model directory

    Returns:

    """
    model_dir = Path(model_dir)

    if not model_dir.exists():
        raise RuntimeError("model directory {dir} does not exist".format(dir=model_dir))

    config_path = model_dir/"pipeline.config"
    checkpoint_path = model_dir/"model.ckpt"
    export_dir = model_dir/"exported_model"

    config = pipeline_pb2.TrainEvalPipelineConfig()
    with open(str(config_path), 'r') as f:
        text_format.Merge(f.read(), config, allow_unknown_extension=True)

    tf_config = tf.ConfigProto()
    tf_config.gpu_options.allow_growth = True

    with tf.Session(config=tf_config):
        with tf.Graph().as_default():
            exporter.export_inference_graph(
                "image_tensor",
                config,
                str(checkpoint_path),
                str(export_dir),
                input_shape=[1, 300, 300, 3]) # fix input shape
            
            
# Export model to ssd_mobilenet_v2_coco_2018_03_29/exported_model 
model_dir = "ssd_mobilenet_v2_coco_2018_03_29"
export_detection_model(model_dir)

"""Now we are ready to run the model. The following script downloads and preprocess an image from internet. Then it load SSD MobileNet V2 model in TensorFlow,  perform inference and record inference time. Note that we run it multiple times to make sure we get meaningful statistic. Finally, we perform sanity check to make sure our model yield correct results."""

import time
import requests
import json

from PIL import Image, ImageDraw
from io import BytesIO
from tensorflow.python.client import timeline

def load_graph_def(model_path) -> tf.GraphDef:
    """
    Helper function to read model protobuf and return graph definition
    Args:
        model_path: Path to frozen model .pb file

    Returns:
        graph_def: graph definition

    """
    graph_def = tf.GraphDef()
    with open(model_path, "rb") as f:
        graph_def.ParseFromString(f.read())

    return graph_def
  
  
def get_iamge_by_url(url: str):
  """ Get image data from url"""
  res = requests.get(url)
  return Image.open(BytesIO(res.content))


def draw_bbox_and_label_in_image(img, boxes, num_detections, box_width=3):
    """
    Draw bounding boxes and class labels in images
    :param img: PIL Image or np arrays
    :param boxes: in size [num_detections, 4], contains xys or boxes
    :param box_width: the width of boxes
    :return: Image
    """

    draw = ImageDraw.Draw(img)
    width, height = img.size

    for i in range(num_detections):
        ymin, xmin, ymax, xmax = boxes[i]

        ymin = int(ymin * height)
        ymax = int(ymax * height)
        xmin = int(xmin * width)
        xmax = int(xmax * width)

        class_color = "LimeGreen"

        draw.line([(xmin, ymin), (xmax, ymin), (xmax, ymax), (xmin, ymax), (xmin, ymin)], width=box_width, fill=class_color)

    return img
  

def run_ssd_mobilenet_v2_tf(image: Image, frozen_model_path: str, record_trace=False, trace_filename="ssd_mobilenet_v2.json"):
  """ Run the model and report the average inference time, return inference time and output for sanity check """
  input_tensor_name = "image_tensor:0"
  output_tensor_names = ['detection_boxes:0', 'detection_classes:0', 'detection_scores:0', 'num_detections:0']
  ssd_mobilenet_v2_graph_def = load_graph_def(frozen_model_path)
  
  # Preprocess the image
  image_tensor = image.resize((300, 300))
  image_tensor = np.array(image_tensor)
  image_tensor = np.expand_dims(image_tensor, axis=0)

  with tf.Graph().as_default() as g:
    tf.import_graph_def(ssd_mobilenet_v2_graph_def, name='')
    input_tensor = g.get_tensor_by_name(input_tensor_name)
    output_tensors = [g.get_tensor_by_name(name) for name in output_tensor_names]

    with tf.Session(graph=g) as sess:
      # The first run will generally take longer, so we feed some random data
      # to warm up the session
      sess.run(output_tensors, feed_dict={input_tensor: np.random.randint(0, 255, (1, 300, 300, 3))})

      # Add metadata to record runtime performance for further analysis
      if record_trace:
        options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)
        run_metadata = tf.RunMetadata()
      else:
        options = None
        run_metadata = None

      inference_time = []

      if record_trace:
        start = time.time()
        outputs = sess.run(output_tensors, feed_dict={input_tensor: image_tensor},
                           options=options, run_metadata=run_metadata)
        inference_time.append((time.time()-start)*1000.) # in ms
        
        # Write metadata
        fetched_timeline = timeline.Timeline(run_metadata.step_stats)
        chrome_trace = fetched_timeline.generate_chrome_trace_format()

        with open('ssd_mobilenet_v2_coco_2018_03_29/exported_model/' + trace_filename, 'w') as f:
            f.write(chrome_trace)
      else:
        # Run multiple times to get meaningful statistic
        for i in range(30):
          start = time.time()
          outputs = sess.run(output_tensors, feed_dict={input_tensor: image_tensor})
          inference_time.append((time.time()-start)*1000.) # in ms
        print("SSD MobileNet V2 model inference time: %.2f ms" % np.mean(inference_time))


  return outputs, inference_time
  

# Download image
ssd_mobilenet_v2_origin_path = "ssd_mobilenet_v2_coco_2018_03_29/exported_model/frozen_inference_graph.pb"
image = get_iamge_by_url("https://www.rover.com/blog/wp-content/uploads/2017/05/pug-tilt-960x540.jpg")

# Run test
outputs, inference_time = run_ssd_mobilenet_v2_tf(image, ssd_mobilenet_v2_origin_path)

# Save resutl to disk, for simplicity, we use JSON here
tf_result = dict(inference_time=inference_time)
with open("tf_result.json", 'w+') as f:
  f.write(json.dumps(tf_result))

# Sanity check
draw_image = draw_bbox_and_label_in_image(image, outputs[0][0], int(outputs[3][0]))
draw_image

"""If you run the code properly, you should see that the inference time for raw SSD MobileNet V2 TensorFlow model is ~20ms and 4 bounding boxes showing the position of 4 dogs. We also save the inference time to disk for further comparsion. 


### Optimize and benchmark optimized model in TensorFlow

In this section, we optimzie the raw SSD MobileNet V2 model by forcing the Non-Maximum Suppression operations running on CPU. For detail explanation, please see the origin article. Then we run the optimzied model in TensorFlow and record the inference time.
"""

# Export optimized model
ssd_mobilenet_v2_optimized_graph_def = load_graph_def(ssd_mobilenet_v2_origin_path)
for node in ssd_mobilenet_v2_optimized_graph_def.node:
  if 'NonMaxSuppression' in node.name:
    node.device = '/device:CPU:0'

with open("ssd_mobilenet_v2_coco_2018_03_29/exported_model/frozen_inference_graph_optimized.pb", "wb") as f:
    f.write(ssd_mobilenet_v2_optimized_graph_def.SerializeToString())
    
# Run test
ssd_mobilenet_v2_optimized_path = "ssd_mobilenet_v2_coco_2018_03_29/exported_model/frozen_inference_graph_optimized.pb"
image = get_iamge_by_url("https://www.rover.com/blog/wp-content/uploads/2017/05/pug-tilt-960x540.jpg")

outputs, inference_time = run_ssd_mobilenet_v2_tf(image, ssd_mobilenet_v2_optimized_path)

# Save running results
tf_optimized_result = dict(inference_time=inference_time)
with open("tf_optimized_result.json", 'w+') as f:
  f.write(json.dumps(tf_optimized_result))

"""You should see that the inference time for our optimized model is ~15 ms. We have made the SSD MobileNet V2 model run 1.3 faster than the origin version! 

### Convert TensorFlow models to TensorRT and benchmark the model in TensorRT

This section shows how we convert the SSD MobileNet V2 TensorFlow model to TensorRT model using Graph Surgeon and UFF converter. For some simple models (e.g. Mobilenet V2, Inception v4 for image classification), we can convert using UFF converter directly. Howerver, for models contaisn operations not supported by TensorRT (e.g. Non-Maximum Suppression in SSD MobileNet V2), we have to do some preprocesses. The trick is using Graph Surgeon to replace the unsupported operations with supported operations. 

The following script provide a preprocess function and modify the origin graph. The key operation is replace the Non-Maximum Suppression operations in the orgin  graph with `NMS_TRT` operation, which perform similar operation and is supported by TensorRT. Then it pass the modified graph to UFF converter and output the final UFF model.
"""

import ctypes
import uff
import tensorrt as trt
import graphsurgeon as gs
import pycuda.driver as cuda
import pycuda.autoinit

ctypes.CDLL("build/libflattenconcat.so")

# Preprocess function to convert TF model to UFF
def ssd_mobilenet_v2_unsupported_nodes_to_plugin_nodes(ssd_graph, input_shape):
    """Makes ssd_graph TensorRT comparible using graphsurgeon.

    This function takes ssd_graph, which contains graphsurgeon
    DynamicGraph data structure. This structure describes frozen Tensorflow
    graph, that can be modified using graphsurgeon (by deleting, adding,
    replacing certain nodes). The graph is modified by removing
    Tensorflow operations that are not supported by TensorRT's UffParser
    and replacing them with custom layer plugin nodes.

    Note: This specific implementation works only for
    ssd_mobilenet_v2_coco_2018_03_29 network.

    Args:
        ssd_graph (gs.DynamicGraph): graph to convert
        input_shape: input shape in CHW format
    Returns:
        gs.DynamicGraph: UffParser compatible SSD graph
    """

    channels, height, width = input_shape

    Input = gs.create_plugin_node(name="Input",
        op="Placeholder",
        dtype=tf.float32,
        shape=[1, channels, height, width])
    PriorBox = gs.create_plugin_node(name="GridAnchor", op="GridAnchor_TRT",
        minSize=0.2,
        maxSize=0.95,
        aspectRatios=[1.0, 2.0, 0.5, 3.0, 0.33],
        variance=[0.1,0.1,0.2,0.2],
        featureMapShapes=[19, 10, 5, 3, 2, 1],
        numLayers=6
    )
    NMS = gs.create_plugin_node(
        name="NMS",
        op="NMS_TRT",
        shareLocation=1,
        varianceEncodedInTarget=0,
        backgroundLabelId=0,
        confidenceThreshold=1e-8,
        nmsThreshold=0.6,
        topK=100,
        keepTopK=100,
        numClasses=91,
        inputOrder=[1, 0, 2],
        confSigmoid=1,
        isNormalized=1
    )
    concat_priorbox = gs.create_node(
        "concat_priorbox",
        op="ConcatV2",
        dtype=tf.float32,
        axis=2
    )
    concat_box_loc = gs.create_plugin_node(
        "concat_box_loc",
        op="FlattenConcat_TRT",
        dtype=tf.float32,
        axis=1,
        ignoreBatch=0
    )
    concat_box_conf = gs.create_plugin_node(
        "concat_box_conf",
        op="FlattenConcat_TRT",
        dtype=tf.float32,
        axis=1,
        ignoreBatch=0
    )

    # Create a mapping of namespace names -> plugin nodes.
    namespace_plugin_map = {
        "MultipleGridAnchorGenerator": PriorBox,
        "Postprocessor": NMS,
        "Preprocessor/map": Input,
        "ToFloat": Input,
        # "image_tensor": Input,
        "Concatenate": concat_priorbox,
        "concat": concat_box_loc,
        "concat_1": concat_box_conf
    }
    for node in ssd_graph.graph_inputs:
        namespace_plugin_map[node.name] = Input

    # Create a new graph by collapsing namespaces
    ssd_graph.collapse_namespaces(namespace_plugin_map)
    # Remove the outputs, so we just have a single output node (NMS).
    # If remove_exclusive_dependencies is True, the whole graph will be removed!
    ssd_graph.remove(ssd_graph.graph_outputs, remove_exclusive_dependencies=False)
    # Disconnect the Input node from NMS, as it expects to have only 3 inputs.
    ssd_graph.find_nodes_by_op("NMS_TRT")[0].input.remove("Input")
    
    return ssd_graph

  
# Simple helper data class that's a little nicer to use than a 2-tuple.
class HostDeviceMem(object):
    def __init__(self, host_mem, device_mem):
        self.host = host_mem
        self.device = device_mem

    def __str__(self):
        return "Host:\n" + str(self.host) + "\nDevice:\n" + str(self.device)

    def __repr__(self):
        return self.__str__()
      
      
def allocate_buffers(engine):
    """Allocates host and device buffer for TRT engine inference.

    This function is similair to the one in ../../common.py, but
    converts network outputs (which are np.float32) appropriately
    before writing them to Python buffer. This is needed, since
    TensorRT plugins doesn't support output type description, and
    in our particular case, we use NMS plugin as network output.

    Args:
        engine (trt.ICudaEngine): TensorRT engine

    Returns:
        inputs [HostDeviceMem]: engine input memory
        outputs [HostDeviceMem]: engine output memory
        bindings [int]: buffer to device bindings
        stream (cuda.Stream): cuda stream for engine inference synchronization
    """
    inputs = []
    outputs = []
    bindings = []
    stream = cuda.Stream()

    # Current NMS implementation in TRT only supports DataType.FLOAT but
    # it may change in the future, which could brake this sample here
    # when using lower precision [e.g. NMS output would not be np.float32
    # anymore, even though this is assumed in binding_to_type]
    binding_to_type = {"Input": np.float32, "NMS": np.float32, "NMS_1": np.int32}

    for binding in engine:
        size = trt.volume(engine.get_binding_shape(binding)) * engine.max_batch_size
        dtype = binding_to_type[str(binding)]
        # Allocate host and device buffers
        host_mem = cuda.pagelocked_empty(size, dtype)
        device_mem = cuda.mem_alloc(host_mem.nbytes)
        # Append the device buffer to device bindings.
        bindings.append(int(device_mem))
        # Append to the appropriate list.
        if engine.binding_is_input(binding):
            inputs.append(HostDeviceMem(host_mem, device_mem))
        else:
            outputs.append(HostDeviceMem(host_mem, device_mem))
    return inputs, outputs, bindings, stream
  
# Export UFF model file
ssd_mobilenet_v2_pb_path = "ssd_mobilenet_v2_coco_2018_03_29/frozen_inference_graph.pb"
output_uff_filename = "ssd_mobilenet_v2_coco_2018_03_29/frozen_inference_graph.uff"
input_shape = (3, 300, 300)

dynamic_graph = gs.DynamicGraph(ssd_mobilenet_v2_pb_path)
dynamic_graph = ssd_mobilenet_v2_unsupported_nodes_to_plugin_nodes(dynamic_graph, input_shape)

uff.from_tensorflow(dynamic_graph.as_graph_def(), output_nodes=["NMS"], output_filename=output_uff_filename)

"""Now we have the UFF model file. We are ready to build TensorRT engine, which optimze your model for the device you run. Thus, it's recommended to re-build the engine for different devices."""

# Build TensorRT engine
uff_model_path = "ssd_mobilenet_v2_coco_2018_03_29/frozen_inference_graph.uff"
engine_path = "ssd_mobilenet_v2_coco_2018_03_29/ssd_mobilenet_v2_bs_1.engine"
TRT_LOGGER = trt.Logger(trt.Logger.WARNING)
trt.init_libnvinfer_plugins(TRT_LOGGER, '')

trt_runtime = trt.Runtime(TRT_LOGGER)

with trt.Builder(TRT_LOGGER) as builder, builder.create_network() as network, trt.UffParser() as parser:
  builder.max_workspace_size = 1 << 30
  builder.fp16_mode = True
  builder.max_batch_size = 1
  parser.register_input("Input", (3, 300, 300))
  parser.register_output("MarkOutput_0")
  parser.parse(uff_model_path, network)
  
  print("Building TensorRT engine, this may take a few minutes...")
  trt_engine = builder.build_cuda_engine(network)

# Save engine for further use
buf = trt_engine.serialize()
with open(engine_path, 'wb') as f:
    f.write(buf)
    
# Load engine from file
with open(engine_path, 'rb') as f:
    engine_data = f.read()
trt_engine = trt_runtime.deserialize_cuda_engine(engine_data)

"""Now we have TensorRT engine, we are ready to run our model on TensorRT. Note that TensorRT requires input images in NCHW format. Thus, our input format should be `[1, 3, 300, 300]` instead of  `[1, 300, 300, 3]` in TensorFlow."""

image = get_iamge_by_url("https://www.rover.com/blog/wp-content/uploads/2017/05/pug-tilt-960x540.jpg")
image_tensor = image.resize((300, 300))
image_tensor = np.array(image_tensor)
image_tensor = np.transpose(image_tensor, (2, 0, 1))
image_tensor = image_tensor.ravel()

inputs, outputs, bindings, stream = allocate_buffers(trt_engine)
exec_context = trt_engine.create_execution_context()

def run_trt_model(image_tensor, exec_context, inputs, outputs, bindings, stream):
  
  # Copy input to appropriate place
  np.copyto(inputs[0].host, image_tensor)

  start = time.time()
  # Copy input from host memory to GPU memory
  for inp in inputs:
    cuda.memcpy_htod_async(inp.device, inp.host, stream)

  # Run inference
  exec_context.execute_async(batch_size=1, bindings=bindings, stream_handle=stream.handle)

  # Copy result from GPU memory to host memory
  for out in outputs:
    cuda.memcpy_dtoh_async(out.host, out.device, stream)

  stream.synchronize()
  inference_time = (time.time()-start)*1000.
  
  res = [out.host for out in outputs]
  return res, inference_time

inference_time = []
for i in range(30):
  res, t = run_trt_model(image_tensor, exec_context, inputs, outputs, bindings, stream)
  inference_time.append(t)

print("TensorRT inference time: %.2f ms" % np.mean(inference_time))
  
# Write to files
trt_result = dict(inference_time=inference_time)
with open("trt_result.json", 'w+') as f:
  f.write(json.dumps(trt_result))
  
res = res[0].reshape((100, 7))
num_detections = np.sum(res[:, 2] > 0.5)

# Sanity check
draw_image = draw_bbox_and_label_in_image(image, res[:, 3:][:, [3, 2, 1, 0]], num_detections)
draw_image

"""You should see that the inference time for TensorRT is ~5 ms. We also do sanity check to make sure the result is correct. 

### Compare the resutls and conclusion

Finally we comapre the inference time for all of models we run. Running the following script and see the result.
"""

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
import pandas as pd

# %matplotlib inline

# Load running results
with open("tf_result.json", 'r') as f:
  tf_result = json.loads(f.read())
  
with open("tf_optimized_result.json", 'r') as f:
  tf_optimized_result = json.loads(f.read())
  
with open("trt_result.json", 'r') as f:
  trt_result = json.loads(f.read())
  
results = dict(TensorFlow=tf_result["inference_time"],
               TensorFlow_Optimized=tf_optimized_result["inference_time"],
               TensorRT=trt_result["inference_time"])

results = pd.DataFrame(data=results)

avg_inference_time = results.mean()
baseline = avg_inference_time["TensorFlow"]

# Plot result
fig, ax = plt.subplots(figsize=(10, 8))

bar_plot = avg_inference_time.plot.bar(ax=ax)

for p in ax.patches:
  ax.annotate("%.2fx" % (baseline / p.get_height()), (p.get_x() + p.get_width()/2, p.get_height() + .5),
              ha="center", va="center", weight="bold")
  
ax.set_ylabel("Inference Time (ms)", fontsize=16)

